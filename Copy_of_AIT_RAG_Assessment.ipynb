{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e14f9e4c7c240f48131cb4486c81dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba605db1d5cf4db092895c0c894e3917",
              "IPY_MODEL_133c874cb2c64e2a888c6dd587743a8a",
              "IPY_MODEL_a13bedaf3a8c409baf3b44be39888265"
            ],
            "layout": "IPY_MODEL_a4d9f2aac63c4edaa04c63b89e0f8e30"
          }
        },
        "ba605db1d5cf4db092895c0c894e3917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ef0887932c40ccb3c0b022b6f5ad29",
            "placeholder": "​",
            "style": "IPY_MODEL_c1895bd55e4e42cb8948cdcb9da0c0bc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "133c874cb2c64e2a888c6dd587743a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8487a1eaad4ca0907abdddf238f6c7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea2e4621c6a4ce0b7bbfe6410c82567",
            "value": 4
          }
        },
        "a13bedaf3a8c409baf3b44be39888265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19042086532d4751bebb2005b006584e",
            "placeholder": "​",
            "style": "IPY_MODEL_27c069f2570148b6a9b362b643619525",
            "value": " 4/4 [01:43&lt;00:00, 22.52s/it]"
          }
        },
        "a4d9f2aac63c4edaa04c63b89e0f8e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ef0887932c40ccb3c0b022b6f5ad29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1895bd55e4e42cb8948cdcb9da0c0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c8487a1eaad4ca0907abdddf238f6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea2e4621c6a4ce0b7bbfe6410c82567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19042086532d4751bebb2005b006584e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c069f2570148b6a9b362b643619525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliza-giane/dl-assessment7/blob/main/Copy_of_AIT_RAG_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + LLM Assessment"
      ],
      "metadata": {
        "id": "e1xCp6n57BB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ],
      "metadata": {
        "id": "qmI3jBxK68-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documents or,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ],
      "metadata": {
        "id": "K3STgq_s6_mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.1. INSTALL DEPENDENCIES:"
      ],
      "metadata": {
        "id": "rUv3Xdf6i14v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null --use-deprecated=legacy-resolver\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null --use-deprecated=legacy-resolver\n",
        "!pip install langchain > null --use-deprecated=legacy-resolver\n",
        "!pip install chromadb > null --use-deprecated=legacy-resolver\n",
        "!pip install sentence_transformers > null --use-deprecated=legacy-resolver # ==2.2.2\n",
        "!pip install unstructured > null --use-deprecated=legacy-resolver\n",
        "!pip install pdf2image > null --use-deprecated=legacy-resolver\n",
        "!pip install pdfminer.six > null --use-deprecated=legacy-resolver\n",
        "!pip install unstructured-pytesseract > null --use-deprecated=legacy-resolver\n",
        "!pip install unstructured-inference > null --use-deprecated=legacy-resolver\n",
        "!pip install faiss-gpu > null --use-deprecated=legacy-resolver\n",
        "!pip install pikepdf > null --use-deprecated=legacy-resolver\n",
        "!pip install pypdf > null --use-deprecated=legacy-resolver\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null --use-deprecated=legacy-resolver\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n"
      ],
      "metadata": {
        "id": "GrkzEsf3jHJH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESTART KERNEL:\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "KktK1vcFjPB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0.2. IMPORTS:"
      ],
      "metadata": {
        "id": "ePvitT8EjVEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ],
      "metadata": {
        "id": "Poeo6q89jYQc",
        "outputId": "3ce930e4-7941-4e49-c971-2ea7b9fdada3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. COLLECT DATASET"
      ],
      "metadata": {
        "id": "5dgwhW4Um-fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING DATASET CUTOFF DATE: March 2023\n",
        "# DATE OF COLLECTED DATASET: 2024\n",
        "\n",
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.04403\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.04345\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.04251\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.04097\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.04390\""
      ],
      "metadata": {
        "id": "5uWphmr_nEWs",
        "outputId": "a57a3400-a541-445d-c80f-6d71497a5f28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-08 20:08:03--  https://arxiv.org/pdf/2405.04403\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 861504 (841K) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "document1.pdf       100%[===================>] 841.31K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-08 20:08:03 (15.1 MB/s) - ‘document1.pdf’ saved [861504/861504]\n",
            "\n",
            "--2024-05-08 20:08:03--  https://arxiv.org/pdf/2405.04345\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9154840 (8.7M) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>]   8.73M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-08 20:08:03 (74.1 MB/s) - ‘document2.pdf’ saved [9154840/9154840]\n",
            "\n",
            "--2024-05-08 20:08:03--  https://arxiv.org/pdf/2405.04251\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 721398 (704K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 704.49K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-08 20:08:03 (13.0 MB/s) - ‘document3.pdf’ saved [721398/721398]\n",
            "\n",
            "--2024-05-08 20:08:03--  https://arxiv.org/pdf/2405.04097\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12652864 (12M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]  12.07M  64.3MB/s    in 0.2s    \n",
            "\n",
            "2024-05-08 20:08:04 (64.3 MB/s) - ‘document4.pdf’ saved [12652864/12652864]\n",
            "\n",
            "--2024-05-08 20:08:04--  https://arxiv.org/pdf/2405.04390\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5434658 (5.2M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   5.18M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-05-08 20:08:04 (56.4 MB/s) - ‘document5.pdf’ saved [5434658/5434658]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD:\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "import os\n",
        "loaders = [UnstructuredPDFLoader(fn) for fn in [\"/content/document1.pdf\",\"/content/document2.pdf\",\"/content/document3.pdf\",\"/content/document4.pdf\",\"/content/document5.pdf\"]]"
      ],
      "metadata": {
        "id": "Y0nO_1ftVWoA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. WRITE PROMPTS"
      ],
      "metadata": {
        "id": "dwt4xXWlpY70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RELEVANT PROMPTS:\n",
        "prompt_1 = \"Summarize the paper written by scientists Chen Min about 4D Pre-trained Scene Understanding\"\n",
        "prompt_2 = \"What were the findings of the paper written by Ammarah Hashmi, Sahibzada Adil Shahzad, et al, about understanding human perception of audiovisual Deepfakes\"\n",
        "prompt_3 = \"Summarize the paper written by scientists Somayeh Malekshahi, Javad M. Kheyridoost, and Omid Fatemi on the implementation and evaluation of general models for Detecting Learner Engagement\"\n",
        "\n",
        "\n",
        "# IRRELEVANT PROMPT:\n",
        "irr_prompt = 'What was Rick Riordan wearing when Brasil won the FIFA World Cup in 2002?'"
      ],
      "metadata": {
        "id": "LJ0PeicDpf7B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. LOAD LLM"
      ],
      "metadata": {
        "id": "2X6vwRbNjzHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# must have at least 5B parameters\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8B language model from Meta AI\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "tXjywwptj1dR",
        "outputId": "b5d7b3f5-f29a-4090-9300-57dc484b0d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "7e14f9e4c7c240f48131cb4486c81dc8",
            "ba605db1d5cf4db092895c0c894e3917",
            "133c874cb2c64e2a888c6dd587743a8a",
            "a13bedaf3a8c409baf3b44be39888265",
            "a4d9f2aac63c4edaa04c63b89e0f8e30",
            "d8ef0887932c40ccb3c0b022b6f5ad29",
            "c1895bd55e4e42cb8948cdcb9da0c0bc",
            "4c8487a1eaad4ca0907abdddf238f6c7",
            "5ea2e4621c6a4ce0b7bbfe6410c82567",
            "19042086532d4751bebb2005b006584e",
            "27c069f2570148b6a9b362b643619525"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e14f9e4c7c240f48131cb4486c81dc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. TEST MODEL WITH THE PROMPTS"
      ],
      "metadata": {
        "id": "1Cb0lfSRkTuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# without the collected dataset, the model should fail at answering the questions\n",
        "# change either the prompts or the dataset if it is able to answer them\n",
        "\n",
        "template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "9nwufKe08D3e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [prompt_1, prompt_2, prompt_3, irr_prompt]\n",
        "i = 1\n",
        "for text in prompts:\n",
        "  result = llm(prompt.format(text=text))\n",
        "  print(\"\\n\\nPROMPT\", str(i) + \":\")\n",
        "  print(fill(result.strip(), width=100))\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icZ5vEvuTg79",
        "outputId": "75097de0-b96a-433e-c429-1dd7b98ab56a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 1:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Summarize the paper written by scientists\n",
            "Chen Min about 4D Pre-trained Scene\n",
            "Understanding<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but I couldn't\n",
            "find any information on a specific paper written by scientist Chen Min about \"4D Pre-trained Scene\n",
            "Understanding\". It's possible that the paper may not be widely known or published, or it could be a\n",
            "misspelling or incorrect name.  However, I can provide some general information on the topic of 4D\n",
            "scene understanding and pre-training in computer vision.   Scene understanding is a fundamental task\n",
            "in computer vision, which involves recognizing and interpreting the objects, actions, and\n",
            "relationships within an image or video sequence. In recent years, there has been growing interest in\n",
            "developing methods for 4D scene understanding, which refers to the ability to understand scenes\n",
            "across multiple frames and dimensions (e.g., time, depth).  Pre-training is a popular approach in\n",
            "deep learning, where a model is first trained on a large dataset and then fine-tuned on a target\n",
            "task. In the context of 4D scene understanding, pre-training can involve training a model on a large\n",
            "dataset of videos or images with annotated object detection, tracking, or segmentation labels. This\n",
            "allows the model to learn generalizable features and representations that can be applied to various\n",
            "downstream tasks.  Some recent papers have explored pre-training techniques for 4D scene\n",
            "understanding, such as:  * \"Learning to See in the Fourth Dimension\" by Wang et al. (2020), which\n",
            "proposes a method for pre-training a model using a combination of 2D and 3D convolutional neural\n",
            "networks (CNNs) to learn spatiotemporal features. * \"4D-Net: A Deep Learning Framework for 4D Scene\n",
            "Understanding\" by Li et al. (2020), which presents a framework for pre-training a model using a\n",
            "combination of 2D and 3D CNNs, as well as recurrent neural networks (RNNs) to capture temporal\n",
            "dependencies.  If you're looking for more information on this topic, I recommend searching for\n",
            "relevant research papers on academic databases like arXiv, IEEE Xplore, or Google Scholar. You can\n",
            "also try contacting Dr. Chen Min directly if you know their affiliation or contact information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 2:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What were the findings of the paper\n",
            "written by Ammarah Hashmi, Sahibzada Adil Shahzad, et al, about understanding human perception of\n",
            "audiovisual Deepfakes<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but I\n",
            "couldn't find any information on a specific paper written by Ammarah Hashmi, Sahibzada Adil Shahzad,\n",
            "and others about \"understanding human perception of audiovisual Deepfakes\". It's possible that the\n",
            "paper may not exist or is not widely known.  However, there have been several studies published on\n",
            "the topic of human perception of deepfakes, including audiovisual deepfakes. Here are some key\n",
            "findings from recent research:  1. **Perception of authenticity**: Studies have shown that people\n",
            "can be easily deceived by deepfake videos, with many believing them to be real (e.g., [1]). In one\n",
            "study, 86% of participants believed a manipulated video was authentic [2]. 2. **Factors influencing\n",
            "perception**: Factors such as the quality of the manipulation, the familiarity of the speaker or\n",
            "actor, and the context in which the video is presented can influence how believable a deepfake is\n",
            "perceived to be [3]. 3. **Emotional impact**: Deepfakes can have a significant emotional impact on\n",
            "viewers, even if they are aware that the content is fake. For example, a study found that watching a\n",
            "manipulated video of a loved one could elicit strong emotions, including sadness and anxiety [4]. 4.\n",
            "**Cognitive biases**: Research has identified various cognitive biases that contribute to our\n",
            "susceptibility to deepfakes, including confirmation bias, anchoring bias, and the availability\n",
            "heuristic [5].  If you're interested in learning more about this topic, I recommend searching for\n",
            "peer-reviewed articles on academic databases such as Google Scholar or IEEE Xplore.  References:\n",
            "[1] Wang et al. (2020). \"DeepFake: A Survey on Deep Learning-based Audio-Visual Fake Media\n",
            "Detection.\" arXiv preprint arXiv:2010.13441.  [2] Huh et al. (2020). \"The Effectiveness of DeepFake\n",
            "Detection Methods: An Empirical Study.\" Proceedings of the 2020 ACM SIGCHI Conference on Human\n",
            "Factors in Computing Systems, pp. 1-12.  [3] Li et al. (2020). \"Understanding the Perception of\n",
            "DeepFake Videos: An Exploratory Study.\" Journal of Visual Communication, vol. 19, no. 2, pp.\n",
            "141-155.  [4] Zhang et al. (2020). \"The Emotional Impact of DeepFake Videos: An Experimental Study.\"\n",
            "Computers in Human Behavior, vol. 102, article 102764.  [5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 3:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Summarize the paper written by scientists\n",
            "Somayeh Malekshahi, Javad M. Kheyridoost, and Omid Fatemi on the implementation and evaluation of\n",
            "general models for Detecting Learner\n",
            "Engagement<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but I couldn't find\n",
            "any information on a specific paper written by Somayeh Malekshahi, Javad M. Kheyridoost, and Omid\n",
            "Fatemi on \"Implementation and Evaluation of General Models for Detecting Learner Engagement\". It's\n",
            "possible that the paper may not be publicly available or may not have been published yet.  However,\n",
            "I can provide you with some general information on detecting learner engagement in educational\n",
            "settings. Learner engagement is an important aspect of learning, as it has been linked to improved\n",
            "academic performance, increased motivation, and better retention rates. There are various methods\n",
            "and techniques used to detect learner engagement, including:  1. Learning analytics: This involves\n",
            "analyzing data from various sources such as learning management systems, online platforms, and\n",
            "educational software to identify patterns and trends related to learner engagement. 2. Machine\n",
            "learning algorithms: These algorithms can be trained on large datasets to recognize patterns and\n",
            "predict learner engagement based on factors such as time spent on tasks, completion rates, and\n",
            "interaction with course materials. 3. Natural language processing (NLP): NLP can be used to analyze\n",
            "text-based interactions between learners and instructors, such as chat logs, emails, and discussion\n",
            "forum posts, to assess learner engagement. 4. Behavioral analysis: This involves analyzing\n",
            "behavioral data such as clickstream data, mouse movements, and keyboard strokes to understand how\n",
            "learners interact with digital learning environments. 5. Surveys and self-reported measures:\n",
            "Learners can be asked to complete surveys or self-report their level of engagement through\n",
            "questionnaires or rating scales.  The effectiveness of these methods depends on various factors,\n",
            "including the quality of the data, the complexity of the algorithm, and the context in which they\n",
            "are applied.  If you could provide more information about the specific paper you're looking for, I\n",
            "may be able to help you further.\n",
            "\n",
            "\n",
            "PROMPT 4:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What was Rick Riordan wearing when Brasil\n",
            "won the FIFA World Cup in 2002?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I think there\n",
            "may be a few issues with your question!  Firstly, Rick Riordan is an American author of young adult\n",
            "fantasy novels, not a sports personality or athlete. He's best known for his Percy Jackson and the\n",
            "Olympians series.  Secondly, Brazil did win the FIFA World Cup in 2002, but it wasn't exactly a solo\n",
            "effort by Rick Riordan! The Brazilian national team won the tournament on June 30, 2002, by\n",
            "defeating Germany 2-0 in the final match at the International Stadium in Yokohama, Japan.  Lastly,\n",
            "I'm not aware of any information about what Rick Riordan wore during that time, as he wasn't\n",
            "involved in the event in any way. If you're looking for information on the fashion choices of the\n",
            "Brazilian players or other attendees during the 2002 World Cup, I'd be happy to try and help you\n",
            "with that!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. CREATE LangChain-BASED RAG SYSTEM"
      ],
      "metadata": {
        "id": "RqrgW7iNqbLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a vector database from the documents\n",
        "\n",
        "# split to smaller pieces:\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunked_pdf_doc = []\n",
        "\n",
        "for loader in loaders:\n",
        "    print(\"Loading raw document...\" + loader.file_path)\n",
        "    pdf_doc = loader.load()\n",
        "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
        "    print(\"Splitting text...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
        "    chunked_pdf_doc.extend(documents)\n",
        "\n",
        "len(chunked_pdf_doc)"
      ],
      "metadata": {
        "id": "e_nbV8FlqsUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d341d330-ca02-438d-ccbe-d28fd381a787"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw document.../content/document1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document2.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document3.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document4.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document5.pdf\n",
            "Splitting text...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make vectors:\n",
        "%%time\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8AqQvOFXZfh",
        "outputId": "ada7b6ee-4110-4aa9-9053-31d5131852ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.39 s, sys: 608 ms, total: 10 s\n",
            "Wall time: 13.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "rpl4oBYpiKeh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test:\n",
        "%%time\n",
        "\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 10, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "# query = \"What are the risk factors for NVIDIA?\"\n",
        "# result = Chain_pdf.invoke(query)\n",
        "# print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ydpS5GoHu-",
        "outputId": "3fcc19f8-6902-4680-e479-1d58d14e1b77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.38 ms, sys: 0 ns, total: 1.38 ms\n",
            "Wall time: 3.24 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve information:\n",
        "%%time\n",
        "\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbKrZAzYXfsX",
        "outputId": "c2d7125f-69d0-43bb-e23f-ca4b01b593fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 666 µs, sys: 0 ns, total: 666 µs\n",
            "Wall time: 683 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. PROVIDE PROMPTS TO THE RAG SYSTEM"
      ],
      "metadata": {
        "id": "e7F6aif0qtoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the RAG system should return relevant answers to the relevant prompts\n",
        "# and an empty answer to the irrelevant prompt\n",
        "\n",
        "prompt_1 = \"Give a brief summarry of the paper DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving\"\n",
        "prompt_2 = \"What were the findings of the paper written by Ammarah Hashmi, Sahibzada Adil Shahzad, et al, about understanding human perception of audiovisual Deepfakes\"\n",
        "prompt_3 = \"Give a brief summary of the paper 'A General Model for Detecting Learner Engagement:Implementation and Evaluation' written by scientists Somayeh Malekshahi, Javad M. Kheyridoost, and Omid Fatemi on the implementation and evaluation of general models for Detecting Learner Engagement\"\n",
        "irr_prompt = 'What was Rick Riordan wearing when Brasil won the FIFA World Cup in 2002?'\n",
        "prompts = [prompt_1, prompt_2, prompt_3, irr_prompt]\n",
        "i = 1\n",
        "for query in prompts:\n",
        "  result = Chain_pdf.invoke(query)\n",
        "  print(\"\\n\\nPROMPT\", str(i) + \":\")\n",
        "  print(fill(result['result'].strip(), width=100))\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "elc7u00wwige",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d002c5a-ba63-4d75-f18b-e7f7e95ecedc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 1:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  4 2 0 2  y a M 7  ]  V C . s c [  1 v 0 9 3 4 0 . 5 0 4 2 : v i X r a  DriveWorld: 4D\n",
            "Pre-trained Scene Understanding via World Models for Autonomous Driving  Chen Min1,2, Dawei Zhao2*,\n",
            "Liang Xiao2*, Jian Zhao3, Xinli Xu4, Zheng Zhu5 Lei Jin6, Jianshu Li7, Yulan Guo8, Junliang Xing9,\n",
            "Liping Jing10, Yiming Nie2, Bin Dai2 1School of Computer Science, Peking University 2Unmanned\n",
            "Systems Technology Research Center, Defense Innovation Institute 3China Telecom Institute of AI &\n",
            "NPU 4HKUST 5GigaAI 6BUPT 7Ant Group 8SYSU 9THU 10BJTU minchen@stu.pku.edu.cn, adamzdw@163.com,\n",
            "xiaoliang@nudt.edu.cn  (a) Monocular 2D Pre-training 2D Pre-text Task  2D to 3D View Transform(c)\n",
            "Multi-camera 4D Pre-training  4D Pre-text Task  3D Pre-text Task  (b) Multi-camera 3D Pre-training\n",
            "Feature Extraction  2D to 3D View Transform  ...  Abstract  DriveWorld: 4D Pre-trained Scene\n",
            "Understanding via World Models for Autonomous Driving  Supplementary Material  6. Pre-training\n",
            "Objective  logp(y1:T +L, a1:T +L) ≥ L(y1:T +L, a1:T +L; θ, ϕ)  The proposed DriveWorld for 4D\n",
            "driving pre-training en- compasses the following five components:  ≜  T (cid:88)  t=1\n",
            "Eh1:t,s1:t∽q(h1:t,s1:t|o≤t,a<t)[ log p(yt|ht, st, ˆb) (cid:125) (cid:123)(cid:122) (cid:124) past\n",
            "occupancy loss  BEV Representation Model : bt ∼ qϕ(bt | ot)  Stochastic State Model : st ∼ qϕ(st |\n",
            "ht, at−1, ot)  Dynamic Transition Model : st ∼ pθ(st | ht, ˆat−1) Static Propagation Model : ˆb ∼\n",
            "pθ(ˆb | b  ′  )  (4)  + log p(at|ht, st) (cid:124) (cid:125) (cid:123)(cid:122) past action loss\n",
            "[log p(yT +k|hT , sT , ˆb) (cid:125) (cid:123)(cid:122) (cid:124) future occupancy loss  ] +  L\n",
            "(cid:88)  EhT ,sT ∽q(hT ,sT |o≤T ,a<T )  k=1  ] + log p(aT +k|hT , sT ) (cid:125) (cid:123)(cid:122)\n",
            "future action loss  (cid:124)  Action Decoder : ˆat ∼ pθ(ˆat | ht, st)  3D Occupancy Decoder : ˆyt ∼\n",
            "pθ(ˆyt | ht, st, ˆb).  Limitations and Future Work. Currently, the annota- tion of DriveWorld is\n",
            "still based on LiDAR point clouds. to explore self-supervised learning for It vision-centric pre-\n",
            "training. Besides, the effectiveness of DriveWorld has only been validated on the lightweight\n",
            "ResNet101 backbone; it is worthwhile to consider scal- ing up the dataset and the backbone size. We\n",
            "hope the proposed 4D pre-training method can contribute to the de- velopment of the foundation model\n",
            "for autonomous driving.  is essential  References  [1] Ben Agro, Quinlan Sykora, Sergio Casas, and\n",
            "Raquel Urta- sun. Implicit occupancy flow fields for perception and pre- In CVPR, pages 1379–1388,\n",
            "2023. diction in self-driving. 1  learning [22, 59, 66], and autonomous driving [3, 16, 28, 93]. In\n",
            "reinforcement learning, Ha and Schmidhuber [19] proposed that the world model can be trained quickly\n",
            "in an unsupervised manner to learn a compressed spatial and temporal representation of the\n",
            "environment. Methods in [18, 20, 22, 23] presuppose access to rewards and online in- teraction with\n",
            "the environment from predictions in the com- pact latent space of a world model. ContextWM [82] and\n",
            "SWIM [54] pre-train world models with abundant in-the- wild videos for downstream visual tasks. In\n",
            "autonomous driving, Elfes [13] proposed the geometric occupancy grid as a world model for robot\n",
            "perception and navigation in 1989. MILE [27] proposed to build the world model by pre- dicting the\n",
            "future BEV segmentation from high-resolution videos of expert demonstrations for autonomous driving.\n",
            "GAIA-1 [28] and DriveDreamer [76] have constructed gen- erative world models that harness video,\n",
            "text, and action inputs to create realistic driving scenarios\n",
            "using<|eot_id|><|start_header_id|>user<|end_header_id|>  Give a brief summarry of the paper\n",
            "DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous\n",
            "Driving<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have enough information to\n",
            "provide a summary of the paper \"DriveWorld: 4D Pre-trained Scene Understanding via World Models for\n",
            "Autonomous Driving\". The provided text appears to be a snippet from the paper's abstract,\n",
            "introduction, or methodology section, but it does not contain a clear summary of the main\n",
            "contributions, results, or findings of the research.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 2:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  4 2 0 2  y a M 7  ]  V C . s c [  1 v 7 9 0 4 0 . 5 0 4 2 : v i X r a  Unmasking\n",
            "Illusions: Understanding Human Perception of Audiovisual Deepfakes  Ammarah Hashmi, Sahibzada Adil\n",
            "Shahzad, Chia-Wen Lin, Fellow, IEEE, and Yu Tsao, Senior Member, IEEE, Hsin-Min Wang, Senior Member,\n",
            "IEEE  V. LIMITATIONS  We are the first to offer insights into the study of human perception of\n",
            "audiovisual deepfakes. In addition to its contribu- tion, this study has several limitations that\n",
            "can be mitigated in future studies to thoroughly explore this field. First, our choice to use a\n",
            "sample size of 40 may not be sufficient to draw general conclusions about human perception in\n",
            "audiovisual deepfake detection tasks. Second, other interventions, such as control situations or\n",
            "providing incentives, can be used to further explore human behavior. Third, to expand the language\n",
            "range, experiments can be conducted in other languages. Fourth, participants can be given multiple\n",
            "samples with different manipulation types to evaluate their performance.  VI. CONCLUSIONS AND FUTURE\n",
            "WORK  At first, audiovisual deepfakes may appear convincing; however, AI-generated content can\n",
            "exhibit artifacts, such as irregular shadows, hair anomalies, speech artifacts, glitches, or\n",
            "inconsistencies. An effective approach to enhance par- ticipants’ performance in detecting\n",
            "audiovisual deepfakes is raising awareness about these deceptive techniques and their adverse\n",
            "effects. Surprisingly, our investigation shows that forewarning of these artifacts and the potential\n",
            "adverse ef- fects of audiovisual deepfake technology did not have any significant impact on\n",
            "participants’ performance. As shown in Fig. 12, participants who received a warning showed similar\n",
            "performance to those who did not receive a warning. Not only did the two groups of participants have\n",
            "similar median accuracy/confidence, but the distributions were also similar. The similar performance\n",
            "in both scenarios may be due to human visual and auditory perception of content. Humans may have\n",
            "cognitive biases and be overconfident in their ability to detect  To further expand and refine our\n",
            "understanding of human perception of audiovisual deepfakes, we can implement a test design that\n",
            "measures subjective performance in a more discreet and unbiased way. For example, the applicability\n",
            "of our findings can be generalized by including a broader and more diverse population. Detailed\n",
            "analysis of the auditory and visual components of audiovisual signals will reveal how in- dividuals\n",
            "perceive, interpret and process audio and visual cues in multimodal deepfake videos. Furthermore,\n",
            "exploring the impact of the popularity of people in videos (e.g., celebrities or politicians) may\n",
            "provide valuable insights into enhancing<|eot_id|><|start_header_id|>user<|end_header_id|>  What\n",
            "were the findings of the paper written by Ammarah Hashmi, Sahibzada Adil Shahzad, et al, about\n",
            "understanding human perception of audiovisual\n",
            "Deepfakes<|eot_id|><|start_header_id|>assistant<|end_header_id|> According to the provided text, the\n",
            "findings of the paper are:  * Despite being warned about the potential artifacts and adverse effects\n",
            "of audiovisual deepfakes, participants' performance in detecting them was not significantly\n",
            "impacted. * Participants who received a warning showed similar performance to those who did not\n",
            "receive a warning. * The median accuracy and confidence levels were similar for both groups, and the\n",
            "distributions were also similar. * This suggests that humans may have cognitive biases and be\n",
            "overconfident in their ability to detect audiovisual deepfakes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Lin, C.-H., Ma, W.-C., Torralba, A., Lucey, S., 2021. BARF: Bundle-adjusting neural radiance fields. Proceedings of the IEEE/CVF International Conference on Computer Vision, 5741–5751.\\n\\nMildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., Ng, R., 2020. NeRF: Representing scenes as neural radiance fields for view synthesis. Proceedings of the European Conference on Computer Vision, Springer, 405–421.\\n\\nPłaczek, M., Piszczek, Ł., 2018. Testing of an industrial ro- bot’s accuracy and repeatability in off and online environments. Eksploatacja i Niezawodnosc — Maintenance and Reliability, 20(3), 455–464.\\n\\nPoggi, M., Zama Ramirez, P., Tosi, F., Salti, S., Di Stefano, L., Mattoccia, S., 2022. Cross-spectral neural radiance fields. Pro- ceedings of the International Conference on 3D Vision. 3DV.\\n\\nSch¨onberger, J. L., Frahm, J.-M., 2016. Structure-from-motion revisited. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4104–4113.', metadata={'source': '/content/document2.pdf'}), -0.1593452474004422), (Document(page_content='adopted by Nerfacto (Tancik et al., 2023), which additionally incorporates further extensions from various other NeRF methods, such as the refinement of the camera poses from Bundle-Adjusting Neural Radiance Fields (BARFs) (Lin et al., 2021), with the overall goal of creating a compromise between quality and training time.', metadata={'source': '/content/document2.pdf'}), -0.18304383026223858), (Document(page_content='(a) Reference view\\n\\n(b) Ensemble prediction\\n\\n(c) Magnitude of residuals\\n\\n(d) Ensemble standard deviation\\n\\nFigure 8. Ensemble prediction on out-of-distribution evaluation image\\n\\nIn the future, we will extend our analysis from novel view syn- thesis to the task of 3D scene or object reconstruction, respect- ively, in the industrial context. Furthermore, we will also look into the thermal domain for industrial applications. Since de- termining the camera poses and interior orientation with SfM is expected to be more challenging for thermal images, we con- sider the potential of using a robot for pose estimation to be even higher when applied to e.g. multi-modal NeRF approaches (Poggi et al., 2022) in order to enable cross-spectral scene rep- resentation and analysis.\\n\\nReferences\\n\\nAanæs, H., Jensen, R. R., Vogiatzis, G., Tola, E., Dahl, A. B., 2016. Large-scale data for multiple-view stereopsis. Interna- tional Journal of Computer Vision, 120, 153–168.', metadata={'source': '/content/document2.pdf'}), -0.19245808752042004), (Document(page_content='function is commonly parameterized in terms of a neural net- work. Images of a scene can be estimated from the radiance field via volumetric rendering: Each pixel of an oriented image is regarded as a ray intersecting the scene, where the color of a pixel is estimated by integrating over the density and color along the ray. During training, this concept is applied to render images for which there is a real counterpart in the training data. The photometric error, i.e., the per pixel color error between the rendered and the real image, is used to optimize the network parameters. Barron et al. (2021) improve upon this method by regarding pixels as cones instead of rays, which has an anti- aliasing effect and increases rendering quality. This improve- ment is also adopted by Nerfacto (Tancik et al., 2023), which additionally incorporates further extensions from various other NeRF methods, such as the refinement of the camera poses from Bundle-Adjusting Neural Radiance Fields (BARFs) (Lin et al., 2021), with the', metadata={'source': '/content/document2.pdf'}), -0.1949618632030583)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PROMPT 3:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  4 2 0 2  y a M 7  ]  V C . s c [  1 v 1 5 2 4 0 . 5 0 4 2 : v i X r a  A General Model\n",
            "for Detecting Learner Engagement: Implementation and Evaluation  Somayeh Malekshahi*\n",
            "s.malekshahi@ut.ac.ir  Javad M. Kheyridoost javadkheyridoost@gmail.com  Omid Fatemi*\n",
            "ofatemi@ut.ac.ir  ABSTRACT  2  A General Model for Detecting Learner Engagement: Implementation and\n",
            "Evaluation  7  A General Model for Detecting Learner Engagement: Implementation and Evaluation\n",
            "Table 3: Adapting labels of the DAiSEE dataset to two-level engagement detection  Affective State\n",
            "Policy  Engagement  Confusion  Frustration  Engagement; Boredom  Engagement; Confusion  Engagement ≤\n",
            "1 → Disengaged Engagement ≥ 2 → Engaged Conf usion == 0 → Disengaged Conf usion ≥ 1 → Engaged F\n",
            "rustration == 0 → Disengaged F rustration ≥ 1 → Engaged Engagement == 0 → Disengaged Engagement == 3\n",
            "→ Engaged Boredom ≥ 2 → Disengaged Engagement == 0 → Disengaged Engagement == 3 → Engaged ((Conf\n",
            "usion == 0 or Conf usion == 3) and  Engagement == 1) → Disengaged  Engagement; Frustration\n",
            "Otherwise → Engaged Engagement == 0 → Disengaged Engagement == 3 → Engaged ((F rustration == 0 or F\n",
            "rustration == 3) and  Engagement == 1) → Disengaged  Confusion; Frustration  Confusion; Frustration\n",
            "Engagement  10  A General Model for Detecting Learner Engagement: Implementation and Evaluation\n",
            "References  [1] G. Ring and G. Mathieux, “The key components of quality learning,” in ASTD\n",
            "Techknowledge 2002 Conference,  Las Vegas, February 2002.  [2] A. Rovai, “Building sense of\n",
            "community at a distance,” International Review of Research in Open and Distance Learning (IRRODL),\n",
            "vol. 3, no. 1, April 2002. [Online]. Available: http://www.irrodl.org/content/v3.1/rovai.pdf [3] B.\n",
            "Khan, “Web-based instruction: What is it and why is it?” in Web-based Instruction, B. H. Khan, Ed.\n",
            "Englewood  Cliffs, NJ: Educational Technology Publications, 1997, pp. 5–18.  [4] J. Whitehill, Z.\n",
            "Serpell, Y.-C. Lin, A. Foster, and J. R. Movellan, “The faces of engagement: Automatic recognition\n",
            "of student engagement from facial expressions,” IEEE Transactions on Affective Computing, vol. 5,\n",
            "no. 1, pp. 86–98, Jan.-March 2014. [Online]. Available:\n",
            "https://doi.org/10.1109/TAFFC.2014.2316163<|eot_id|><|start_header_id|>user<|end_header_id|>  Give a\n",
            "brief summary of the paper 'A General Model for Detecting Learner Engagement:Implementation and\n",
            "Evaluation' written by scientists Somayeh Malekshahi, Javad M. Kheyridoost, and Omid Fatemi on the\n",
            "implementation and evaluation of general models for Detecting Learner\n",
            "Engagement<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have enough information to\n",
            "provide a summary of the paper as I only have the references and tables provided. The main text of\n",
            "the paper is missing.\n",
            "\n",
            "\n",
            "PROMPT 4:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  <|eot_id|><|start_header_id|>user<|end_header_id|>  What was Rick Riordan wearing when\n",
            "Brasil won the FIFA World Cup in 2002?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I\n",
            "don't have enough information to answer that question. The text doesn't mention Rick Riordan or his\n",
            "clothing choices, and it also doesn't provide information about Brazil winning the FIFA World Cup in\n",
            "2002.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Z5i8zbb33bY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}